{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Page This Documentation will help those who wants to Start Career on Performance Engineering/Testing or Preparing for Interviews. As a Performance Engineer You Should Learn About Performance Concepts : What is Performance Testing? Difference Between Performance Testing and Engineering? What are the Benefits of doing Performance Testing? Understanding Performance Testing terminologies. What all information you need before starting any Performance Test? When is the right time to start Performance Testing? Types of Performance Testing? Performance Testing Tools and Techniques : What all tools are available to do Performance Testing? What is Performance Monitoring? Available tools and techniques for Monitoring? Java Application level Performance Monitoring : Metrics to monitor in any Java Application. How to capture metrics? Database Level Monitoring : What to check on Database side? How to capture metrics on various databases? UI Performance Measurement : What to check on any webpage to determine performance? How to capture metrics?","title":"Home (This Site is Under Construction)"},{"location":"#home-page","text":"This Documentation will help those who wants to Start Career on Performance Engineering/Testing or Preparing for Interviews.","title":"Home Page"},{"location":"#as-a-performance-engineer-you-should-learn-about","text":"","title":"As a Performance Engineer You Should Learn About"},{"location":"#performance-concepts","text":"What is Performance Testing? Difference Between Performance Testing and Engineering? What are the Benefits of doing Performance Testing? Understanding Performance Testing terminologies. What all information you need before starting any Performance Test? When is the right time to start Performance Testing? Types of Performance Testing?","title":"Performance Concepts :"},{"location":"#performance-testing-tools-and-techniques","text":"What all tools are available to do Performance Testing? What is Performance Monitoring? Available tools and techniques for Monitoring?","title":"Performance Testing Tools and Techniques :"},{"location":"#java-application-level-performance-monitoring","text":"Metrics to monitor in any Java Application. How to capture metrics?","title":"Java Application level Performance Monitoring :"},{"location":"#database-level-monitoring","text":"What to check on Database side? How to capture metrics on various databases?","title":"Database Level Monitoring :"},{"location":"#ui-performance-measurement","text":"What to check on any webpage to determine performance? How to capture metrics?","title":"UI Performance Measurement :"},{"location":"LinuxMonitoring/","text":"Linux Monitoring Linux OS has rich set of libraries and mostly easily accessible. There are native libraries/tools which can monitor CPU , Memory , Disk , Network etc. without having license to any expensive Application Monitoring Solution. No Performance Report is considered complete if you are not providing resource utilization numbers or graph. It helps understand the Infrastructure capability and scaling for current load as well as helps to understand and plan the future load. CPU : There are many ways we can extract or monitor CPU Utilization in Linux a. \"top\" - Type \"top\" in command line and hit enter, you will see something like below. \u200b The output from top is divided into two sections. The first few lines give a summary of the system resources including a breakdown of the number of tasks, the CPU statistics, and the current memory usage. Beneath these stats is a live list of the current running processes. This list can be sorted by PID, CPU usage, memory usage, and so on. The CPU line will look something like this: %Cpu(s): 24.8 us, 0.5 sy, 0.0 ni, 73.6 id, 0.4 wa, 0.0 hi, 0.2 si, 0.0 st 24.8 us - This tells us that the processor is spending 24.8% of its time running user space processes . A user space program is any process that doesn't belong to the kernel. Shells, compilers, databases, web servers, and the programs associated with the desktop are all user space processes. If the processor isn't idle, it is quite normal that the majority of the CPU time should be spent running user space processes. 73.6 id - Skipping over a few of the other statistics, just for a moment, the id statistic tell us that the processor was idle just over 73% of the time during the last sampling period. The total of the user space percentage - us , the niced percentage - ni , and the idle percentage - id , should be close to 100%. Which it is in this case. If the CPU is spending a more time in the other states then something is probably awry - see the Troubleshooting section below. 0.5 sy - This is the amount of time that the CPU spent running the kernel . All the processes and system resources are handled by the Linux kernel. When a user space process needs something from the system, for example when it needs to allocate memory, perform some I/O, or it needs to create a child process, then the kernel is running. In fact the scheduler itself which determines which process runs next is part of the kernel. The amount of time spent in the kernel should be as low as possible. In this case, just 0.5% of the time given to the different processes was spent in the kernel. This number can peak much higher, especially when there is a lot of I/O happening. 0.0 ni - As mentioned above, the priority level a user space process can be tweaked by adjusting its niceness . The ni stat shows how much time the CPU spent running user space processes that have been niced . On a system where no processes have been niced then the number will be 0. 0.4 wa - Input and output operations, like reading or writing to a disk, are slow compared to the speed of a CPU. Although this operations happen very fast compared to everyday human activities, they are still slow when compared to the performance of a CPU. There are times when the processor has initiated a read or write operation and then it has to wait for the result, but has nothing else to do. In other words it is idle while waiting for an I/O operation to complete . The time the CPU spends in this state is shown by the wa statistic. 0.0 hi & 0.2 si - These two statistics show how much time the processor has spent servicing interrupts . hi is for hardware interrupts, and si is for software interrupts. Hardware interrupts are physical interrupts sent to the CPU from various peripherals like disks and network interfaces. Software interrupts come from processes running on the system. A hardware interrupt will actually cause the CPU to stop what it is doing and go handle the interrupt. A software interrupt doesn't occur at the CPU level, but rather at the kernel level. 0.0 st - This last number only applies to virtual machines. When Linux is running as a virtual machine on a hypervisor, the st (short for stolen) statistic shows how long the virtual CPU has spent waiting for the hypervisor to service another virtual CPU running on a different virtual machine. Since in the real-world these virtual processors are sharing the same physical processor(s) then there will be times when the virtual machine wanted to run but the hypervisor scheduled another virtual machine instead. b. \"vmstat\" vmstat is also one of the inbuilt command in unix, it provides CPU, Memory information as well as information like paging, block IO, disks etc. we can also visualize how many requests are waiting to be served by cpu threads. Command : vmstat [interval] [count] Run command like vmstat 1 10 , this will capture vmstat statistics every 1 sec and will run 10 times. If you want to know more about any command available in unix/linux , run 'man' command. Ex : man vmstat , below the selected part of the 'man' command. Procs r: The number of runnable processes (running or waiting for run time). b: The number of processes in uninterruptible sleep. Memory swpd: the amount of virtual memory used. free: the amount of idle memory. buff: the amount of memory used as buffers. cache: the amount of memory used as cache. inact: the amount of inactive memory. (-a option) active: the amount of active memory. (-a option) Swap si: Amount of memory swapped in from disk (/s). so: Amount of memory swapped to disk (/s). IO bi: Blocks received from a block device (blocks/s). bo: Blocks sent to a block device (blocks/s). System in: The number of interrupts per second, including the clock. cs: The number of context switches per second. CPU These are percentages of total CPU time. us: Time spent running non-kernel code. (user time, including nice time) sy: Time spent running kernel code. (system time) id: Time spent idle. Prior to Linux 2.5.41, this includes IO-wait time. wa: Time spent waiting for IO. Prior to Linux 2.5.41, included in idle. st: Time stolen from a virtual machine. Prior to Linux 2.6.11, unknown. c. \"mpstat\" : Some times its required to understand the CPU usage behavior per core. This can be achieved by using 'mpstat' command. Run command as mpstat -P ALL 2 5 , this command will print statistics every 2 sec 5 times. output will look like this : As you can see on top right corner , total number of CPU this server has are 2. Multiple rows of output shows statistics for all the cores as average and per core (watch CPU Column) Memory : Disk : Network : Sources : https://scoutapm.com/blog/understanding-linuxs-cpu-stats","title":"Linux Monitoring"},{"location":"LinuxMonitoring/#linux-monitoring","text":"Linux OS has rich set of libraries and mostly easily accessible. There are native libraries/tools which can monitor CPU , Memory , Disk , Network etc. without having license to any expensive Application Monitoring Solution. No Performance Report is considered complete if you are not providing resource utilization numbers or graph. It helps understand the Infrastructure capability and scaling for current load as well as helps to understand and plan the future load.","title":"Linux Monitoring"},{"location":"LinuxMonitoring/#cpu","text":"There are many ways we can extract or monitor CPU Utilization in Linux a. \"top\" - Type \"top\" in command line and hit enter, you will see something like below. \u200b The output from top is divided into two sections. The first few lines give a summary of the system resources including a breakdown of the number of tasks, the CPU statistics, and the current memory usage. Beneath these stats is a live list of the current running processes. This list can be sorted by PID, CPU usage, memory usage, and so on. The CPU line will look something like this: %Cpu(s): 24.8 us, 0.5 sy, 0.0 ni, 73.6 id, 0.4 wa, 0.0 hi, 0.2 si, 0.0 st 24.8 us - This tells us that the processor is spending 24.8% of its time running user space processes . A user space program is any process that doesn't belong to the kernel. Shells, compilers, databases, web servers, and the programs associated with the desktop are all user space processes. If the processor isn't idle, it is quite normal that the majority of the CPU time should be spent running user space processes. 73.6 id - Skipping over a few of the other statistics, just for a moment, the id statistic tell us that the processor was idle just over 73% of the time during the last sampling period. The total of the user space percentage - us , the niced percentage - ni , and the idle percentage - id , should be close to 100%. Which it is in this case. If the CPU is spending a more time in the other states then something is probably awry - see the Troubleshooting section below. 0.5 sy - This is the amount of time that the CPU spent running the kernel . All the processes and system resources are handled by the Linux kernel. When a user space process needs something from the system, for example when it needs to allocate memory, perform some I/O, or it needs to create a child process, then the kernel is running. In fact the scheduler itself which determines which process runs next is part of the kernel. The amount of time spent in the kernel should be as low as possible. In this case, just 0.5% of the time given to the different processes was spent in the kernel. This number can peak much higher, especially when there is a lot of I/O happening. 0.0 ni - As mentioned above, the priority level a user space process can be tweaked by adjusting its niceness . The ni stat shows how much time the CPU spent running user space processes that have been niced . On a system where no processes have been niced then the number will be 0. 0.4 wa - Input and output operations, like reading or writing to a disk, are slow compared to the speed of a CPU. Although this operations happen very fast compared to everyday human activities, they are still slow when compared to the performance of a CPU. There are times when the processor has initiated a read or write operation and then it has to wait for the result, but has nothing else to do. In other words it is idle while waiting for an I/O operation to complete . The time the CPU spends in this state is shown by the wa statistic. 0.0 hi & 0.2 si - These two statistics show how much time the processor has spent servicing interrupts . hi is for hardware interrupts, and si is for software interrupts. Hardware interrupts are physical interrupts sent to the CPU from various peripherals like disks and network interfaces. Software interrupts come from processes running on the system. A hardware interrupt will actually cause the CPU to stop what it is doing and go handle the interrupt. A software interrupt doesn't occur at the CPU level, but rather at the kernel level. 0.0 st - This last number only applies to virtual machines. When Linux is running as a virtual machine on a hypervisor, the st (short for stolen) statistic shows how long the virtual CPU has spent waiting for the hypervisor to service another virtual CPU running on a different virtual machine. Since in the real-world these virtual processors are sharing the same physical processor(s) then there will be times when the virtual machine wanted to run but the hypervisor scheduled another virtual machine instead. b. \"vmstat\" vmstat is also one of the inbuilt command in unix, it provides CPU, Memory information as well as information like paging, block IO, disks etc. we can also visualize how many requests are waiting to be served by cpu threads. Command : vmstat [interval] [count] Run command like vmstat 1 10 , this will capture vmstat statistics every 1 sec and will run 10 times. If you want to know more about any command available in unix/linux , run 'man' command. Ex : man vmstat , below the selected part of the 'man' command. Procs r: The number of runnable processes (running or waiting for run time). b: The number of processes in uninterruptible sleep. Memory swpd: the amount of virtual memory used. free: the amount of idle memory. buff: the amount of memory used as buffers. cache: the amount of memory used as cache. inact: the amount of inactive memory. (-a option) active: the amount of active memory. (-a option) Swap si: Amount of memory swapped in from disk (/s). so: Amount of memory swapped to disk (/s). IO bi: Blocks received from a block device (blocks/s). bo: Blocks sent to a block device (blocks/s). System in: The number of interrupts per second, including the clock. cs: The number of context switches per second. CPU These are percentages of total CPU time. us: Time spent running non-kernel code. (user time, including nice time) sy: Time spent running kernel code. (system time) id: Time spent idle. Prior to Linux 2.5.41, this includes IO-wait time. wa: Time spent waiting for IO. Prior to Linux 2.5.41, included in idle. st: Time stolen from a virtual machine. Prior to Linux 2.6.11, unknown. c. \"mpstat\" : Some times its required to understand the CPU usage behavior per core. This can be achieved by using 'mpstat' command. Run command as mpstat -P ALL 2 5 , this command will print statistics every 2 sec 5 times. output will look like this : As you can see on top right corner , total number of CPU this server has are 2. Multiple rows of output shows statistics for all the cores as average and per core (watch CPU Column)","title":"CPU  :"},{"location":"LinuxMonitoring/#memory","text":"","title":"Memory :"},{"location":"LinuxMonitoring/#disk","text":"","title":"Disk :"},{"location":"LinuxMonitoring/#network","text":"Sources : https://scoutapm.com/blog/understanding-linuxs-cpu-stats","title":"Network :"},{"location":"PerformanceTestingVSEngineering/","text":"Performance Testing vs Performance Engineering First of all, In my opinion this whole arrangement (Performance Testing and Engineering) is created as the responsibility and applications are growing (in terms of complexity of the Architecture, Functionality and Technologies being used in developing those applications). Now a days we are not limited to application with three tier Architecture kind , applications are developed keeping in mind that there will be many users using the system or much more data is going to come. Micro Services kind Architecture are being developed to deal with such large volumes and users. Usage of highly distributed solutions (NoSql Databases like Cassandra, MongoDB , High Performant Messaging systems like Kafka, Distributed caching Solutions like Memcache , Redis etc) are also helping to achieve such systems. To measure the Performance of such application/software requires understanding of Functionality , Architecture, tools to Monitor as well as Analysis of various end points (Be it infrastructure or technologies being used). Complex UIs and many interconnected services(APIs) are not helping Performance Testing task, its more demanding now to understand the Data Flow, Communication Protocols and various third party software interaction. It requires good amount of time to prepare Data and Performance Scripts to get ready for the Performance Testing especially for large volume. Not just scripting and data preparation its also the Monitoring of resources and application which requires lot of understanding and pre-test setup work. Now a days applications development is fast paced , Agile process has made feature based delivery and deployments. We can't miss any feature or build to consider for its performance behavior. It may perform good or bad or even impact good or bad to existing features as well. Now, What happens if we found that any feature or flow is not performing good? or If any Performance issues comes in Production? What if access of all environment can't be given to everyone or every performance testing team member? Who will work on feature' Performance Testing? at the same time who will be analyzing Production Performance issues? Who will take care of Performance Testing tools, trends and industry standards? What all things to consider for debugging any Performance Issue? What all Profilers are available? How to analyze each layer of Infrastructure or Even Code? All these factors or can say end to end responsibilities are difficult to handle, so somewhere we needed a modularized arrangement where responsibility can be shared or divided and ownership can be defined, hence the 'Performance Testing' and 'Performance Engineering'. So Let's try to understand the responsibilities of each roles. In general Performance Testing is subset of Performance Engineering . Performance Testing is one type of Non Functional Testing , its main goal is to explore the Performance metrics part of application (such as Response Time, Throughput, Scalability) as well as monitoring Resource consumption and behavior under load. Performance Test Engineer has to also take care of things like : Any change in Application Performance after introducing any new feature, improved/degraded or no change? How good or bad our application responds when X number of users will be accessing our application? How many users our application can handle on available size of server with X CPU and Y Memory etc? Can we scale our application in future(in 6 months or 12 months) if application user base or data volume by X rate? Are we seeing any improvement when we move our application from X cloud to Y cloud or from A region to B region? Is there any performance impact if we upgrade our servers or application or dependent technologies? A good Performance Testing professional should also know about what all parameters to monitor and provide the report with all the metrics which is required for analysis. Where as a Performance Engineer might not worry about how to make script or which tool to use for scripting or how to maintain regression test suit or how to prepare data. Its more about analysis/debugging Performance Engineer also has to : Know about the detailed Architecture of the application so that he/she can help Performance Tester to design test case. A good understanding of Architecture also helps to properly understand the Issues or Bottlenecks which is being identified in application during Performance Testing Exercise or in Production. To know about the technologies being used across application and to some extent its internal working and tools/techniques to debug especially its Performance aspects. Help Architects to do POCs based on assumed volume and technologies considered for the application development during design phase itself so that proper adjustment can be made in design to handle the volume and future requirements. Suggest design changes or best practices based on POCs or based on the Historical Performance Test reports pattern. Understanding backend and DB technologies also help to identify Profilers and various methods to track less performing code or queries etc.","title":"Performance Testing vs Performance Engineering"},{"location":"PerformanceTestingVSEngineering/#performance-testing-vs-performance-engineering","text":"First of all, In my opinion this whole arrangement (Performance Testing and Engineering) is created as the responsibility and applications are growing (in terms of complexity of the Architecture, Functionality and Technologies being used in developing those applications). Now a days we are not limited to application with three tier Architecture kind , applications are developed keeping in mind that there will be many users using the system or much more data is going to come. Micro Services kind Architecture are being developed to deal with such large volumes and users. Usage of highly distributed solutions (NoSql Databases like Cassandra, MongoDB , High Performant Messaging systems like Kafka, Distributed caching Solutions like Memcache , Redis etc) are also helping to achieve such systems. To measure the Performance of such application/software requires understanding of Functionality , Architecture, tools to Monitor as well as Analysis of various end points (Be it infrastructure or technologies being used). Complex UIs and many interconnected services(APIs) are not helping Performance Testing task, its more demanding now to understand the Data Flow, Communication Protocols and various third party software interaction. It requires good amount of time to prepare Data and Performance Scripts to get ready for the Performance Testing especially for large volume. Not just scripting and data preparation its also the Monitoring of resources and application which requires lot of understanding and pre-test setup work. Now a days applications development is fast paced , Agile process has made feature based delivery and deployments. We can't miss any feature or build to consider for its performance behavior. It may perform good or bad or even impact good or bad to existing features as well. Now, What happens if we found that any feature or flow is not performing good? or If any Performance issues comes in Production? What if access of all environment can't be given to everyone or every performance testing team member? Who will work on feature' Performance Testing? at the same time who will be analyzing Production Performance issues? Who will take care of Performance Testing tools, trends and industry standards? What all things to consider for debugging any Performance Issue? What all Profilers are available? How to analyze each layer of Infrastructure or Even Code? All these factors or can say end to end responsibilities are difficult to handle, so somewhere we needed a modularized arrangement where responsibility can be shared or divided and ownership can be defined, hence the 'Performance Testing' and 'Performance Engineering'. So Let's try to understand the responsibilities of each roles. In general Performance Testing is subset of Performance Engineering . Performance Testing is one type of Non Functional Testing , its main goal is to explore the Performance metrics part of application (such as Response Time, Throughput, Scalability) as well as monitoring Resource consumption and behavior under load. Performance Test Engineer has to also take care of things like : Any change in Application Performance after introducing any new feature, improved/degraded or no change? How good or bad our application responds when X number of users will be accessing our application? How many users our application can handle on available size of server with X CPU and Y Memory etc? Can we scale our application in future(in 6 months or 12 months) if application user base or data volume by X rate? Are we seeing any improvement when we move our application from X cloud to Y cloud or from A region to B region? Is there any performance impact if we upgrade our servers or application or dependent technologies? A good Performance Testing professional should also know about what all parameters to monitor and provide the report with all the metrics which is required for analysis. Where as a Performance Engineer might not worry about how to make script or which tool to use for scripting or how to maintain regression test suit or how to prepare data. Its more about analysis/debugging Performance Engineer also has to : Know about the detailed Architecture of the application so that he/she can help Performance Tester to design test case. A good understanding of Architecture also helps to properly understand the Issues or Bottlenecks which is being identified in application during Performance Testing Exercise or in Production. To know about the technologies being used across application and to some extent its internal working and tools/techniques to debug especially its Performance aspects. Help Architects to do POCs based on assumed volume and technologies considered for the application development during design phase itself so that proper adjustment can be made in design to handle the volume and future requirements. Suggest design changes or best practices based on POCs or based on the Historical Performance Test reports pattern. Understanding backend and DB technologies also help to identify Profilers and various methods to track less performing code or queries etc.","title":"Performance Testing vs Performance Engineering"},{"location":"jvmarch/","text":"JVM Architecture Every Java developer knows that bytecode will be executed by JRE (Java Runtime Environment). But many doesn't know the fact that JRE is the implementation of Java Virtual Machine (JVM), which analyzes the bytecode, interprets the code, and executes it. It is very important as a developer that we should know the Architecture of the JVM, as it enables us to write code more efficiently. In this article, we will learn more dJVM Architectureeeply about the JVM architecture in Java and the different components of the JVM. What is the JVM? A Virtual Machine is a software implementation of a physical machine. Java was developed with the concept of WORA (Write Once Run Anywhere), which runs on a VM. The compiler compiles the Java file into a Java .class file, then that .class file is input into the JVM, which Loads and executes the class file. Below is a diagram of the Architecture of the JVM. \u200b JVM Architecture Diagram How Does the JVM Work? As shown in the above architecture diagram, the JVM is divided into three main subsystems: Class Loader Subsystem Runtime Data Area Execution Engine 1. Class Loader Subsystem Java's dynamic class loading functionality is handled by the class loader subsystem. It loads, links. and initializes the class file when it refers to a class for the first time at runtime, not compile time. 1.1 Loading Classes will be loaded by this component. Boot Strap class Loader, Extension class Loader, and Application class Loader are the three class loader which will help in achieving it. Boot Strap ClassLoader \u2013 Responsible for loading classes from the bootstrap classpath, nothing but rt.jar. Highest priority will be given to this loader. Extension ClassLoader \u2013 Responsible for loading classes which are inside ext folder (jre\\lib). Application ClassLoader \u2013Responsible for loading Application Level Classpath, path mentioned Environment Variable etc. The above Class Loaders will follow Delegation Hierarchy Algorithm while loading the class files. 1.2 Linking Verify \u2013 Bytecode verifier will verify whether the generated bytecode is proper or not if verification fails we will get the verification error. Prepare \u2013 For all static variables memory will be allocated and assigned with default values. Resolve \u2013 All symbolic memory references are replaced with the original references from Method Area. 1.3 Initialization This is the final phase of Class Loading, here all static variables will be assigned with the original values, and the static block will be executed. 2. Runtime Data Area The Runtime Data Area is divided into 5 major components: Method Area \u2013 All the class level data will be stored here, including static variables. There is only one method area per JVM, and it is a shared resource. Heap Area \u2013 All the Objects and their corresponding instance variables and arrays will be stored here. There is also one Heap Area per JVM. Since the Method and Heap areas share memory for multiple threads, the data stored is not thread safe. Stack Area \u2013 For every thread, a separate runtime stack will be created. For every method call, one entry will be made in the stack memory which is called as Stack Frame. All local variables will be created in the stack memory. The stack area is thread safe since it is not a shared resource. The Stack Frame is divided into three subentities: Local Variable Array \u2013 Related to the method how many local variables are involved and the corresponding values will be stored here. Operand stack \u2013 If any intermediate operation is required to perform, operand stack acts as runtime workspace to perform the operation. Frame data \u2013 All symbols corresponding to the method is stored here. In the case of any exception, the catch block information will be maintained in the frame data. PC Registers \u2013 Each thread will have separate PC Registers, to hold the address of current executing instruction once the instruction is executed the PC register will be updated with the next instruction. Native Method stacks \u2013 Native Method Stack holds native method information. For every thread, a separate native method stack will be created. 3. Execution Engine The bytecode which is assigned to the Runtime Data Area will be executed by the Execution Engine. The Execution Engine reads the bytecode and executes it piece by piece. Interpreter \u2013 The interpreter interprets the bytecode faster, but executes slowly. The disadvantage of the interpreter is that when one method is called multiple times, every time a new interpretation is required. JIT Compiler \u2013 The JIT Compiler neutralizes the disadvantage of the interpreter. The Execution Engine will be using the help of the interpreter in converting byte code, but when it finds repeated code it uses the JIT compiler, which compiles the entire bytecode and changes it to native code. This native code will be used directly for repeated method calls, which improve the performance of the system. Intermediate Code generator \u2013 Produces intermediate code Code Optimizer \u2013 Responsible for optimizing the intermediate code generated above Target Code Generator \u2013 Responsible for Generating Machine Code or Native Code Profiler \u2013 A special component, responsible for finding hotspots, i.e. whether the method is called multiple times or not. Garbage Collector: Collects and removes unreferenced objects. Garbage Collection can be triggered by calling \"System.gc()\", but the execution is not guaranteed. Garbage collection of the JVM collects the objects that are created. Java Native Interface (JNI): JNI will be interacting with the Native Method Libraries and provides the Native Libraries required for the Execution Engine. Native Method Libraries:It is a collection of the Native Libraries which is required for the Execution Engine. Refrences https://dzone.com/articles/jvm-architecture-explained","title":"JVM Architecture"},{"location":"jvmarch/#jvm-architecture","text":"Every Java developer knows that bytecode will be executed by JRE (Java Runtime Environment). But many doesn't know the fact that JRE is the implementation of Java Virtual Machine (JVM), which analyzes the bytecode, interprets the code, and executes it. It is very important as a developer that we should know the Architecture of the JVM, as it enables us to write code more efficiently. In this article, we will learn more dJVM Architectureeeply about the JVM architecture in Java and the different components of the JVM.","title":"JVM Architecture"},{"location":"jvmarch/#what-is-the-jvm","text":"A Virtual Machine is a software implementation of a physical machine. Java was developed with the concept of WORA (Write Once Run Anywhere), which runs on a VM. The compiler compiles the Java file into a Java .class file, then that .class file is input into the JVM, which Loads and executes the class file. Below is a diagram of the Architecture of the JVM. \u200b JVM Architecture Diagram","title":"What is the JVM?"},{"location":"jvmarch/#how-does-the-jvm-work","text":"As shown in the above architecture diagram, the JVM is divided into three main subsystems: Class Loader Subsystem Runtime Data Area Execution Engine 1. Class Loader Subsystem Java's dynamic class loading functionality is handled by the class loader subsystem. It loads, links. and initializes the class file when it refers to a class for the first time at runtime, not compile time. 1.1 Loading Classes will be loaded by this component. Boot Strap class Loader, Extension class Loader, and Application class Loader are the three class loader which will help in achieving it. Boot Strap ClassLoader \u2013 Responsible for loading classes from the bootstrap classpath, nothing but rt.jar. Highest priority will be given to this loader. Extension ClassLoader \u2013 Responsible for loading classes which are inside ext folder (jre\\lib). Application ClassLoader \u2013Responsible for loading Application Level Classpath, path mentioned Environment Variable etc. The above Class Loaders will follow Delegation Hierarchy Algorithm while loading the class files. 1.2 Linking Verify \u2013 Bytecode verifier will verify whether the generated bytecode is proper or not if verification fails we will get the verification error. Prepare \u2013 For all static variables memory will be allocated and assigned with default values. Resolve \u2013 All symbolic memory references are replaced with the original references from Method Area. 1.3 Initialization This is the final phase of Class Loading, here all static variables will be assigned with the original values, and the static block will be executed. 2. Runtime Data Area The Runtime Data Area is divided into 5 major components: Method Area \u2013 All the class level data will be stored here, including static variables. There is only one method area per JVM, and it is a shared resource. Heap Area \u2013 All the Objects and their corresponding instance variables and arrays will be stored here. There is also one Heap Area per JVM. Since the Method and Heap areas share memory for multiple threads, the data stored is not thread safe. Stack Area \u2013 For every thread, a separate runtime stack will be created. For every method call, one entry will be made in the stack memory which is called as Stack Frame. All local variables will be created in the stack memory. The stack area is thread safe since it is not a shared resource. The Stack Frame is divided into three subentities: Local Variable Array \u2013 Related to the method how many local variables are involved and the corresponding values will be stored here. Operand stack \u2013 If any intermediate operation is required to perform, operand stack acts as runtime workspace to perform the operation. Frame data \u2013 All symbols corresponding to the method is stored here. In the case of any exception, the catch block information will be maintained in the frame data. PC Registers \u2013 Each thread will have separate PC Registers, to hold the address of current executing instruction once the instruction is executed the PC register will be updated with the next instruction. Native Method stacks \u2013 Native Method Stack holds native method information. For every thread, a separate native method stack will be created. 3. Execution Engine The bytecode which is assigned to the Runtime Data Area will be executed by the Execution Engine. The Execution Engine reads the bytecode and executes it piece by piece. Interpreter \u2013 The interpreter interprets the bytecode faster, but executes slowly. The disadvantage of the interpreter is that when one method is called multiple times, every time a new interpretation is required. JIT Compiler \u2013 The JIT Compiler neutralizes the disadvantage of the interpreter. The Execution Engine will be using the help of the interpreter in converting byte code, but when it finds repeated code it uses the JIT compiler, which compiles the entire bytecode and changes it to native code. This native code will be used directly for repeated method calls, which improve the performance of the system. Intermediate Code generator \u2013 Produces intermediate code Code Optimizer \u2013 Responsible for optimizing the intermediate code generated above Target Code Generator \u2013 Responsible for Generating Machine Code or Native Code Profiler \u2013 A special component, responsible for finding hotspots, i.e. whether the method is called multiple times or not. Garbage Collector: Collects and removes unreferenced objects. Garbage Collection can be triggered by calling \"System.gc()\", but the execution is not guaranteed. Garbage collection of the JVM collects the objects that are created. Java Native Interface (JNI): JNI will be interacting with the Native Method Libraries and provides the Native Libraries required for the Execution Engine. Native Method Libraries:It is a collection of the Native Libraries which is required for the Execution Engine.","title":"How Does the JVM Work?"},{"location":"jvmarch/#refrences","text":"https://dzone.com/articles/jvm-architecture-explained","title":"Refrences"},{"location":"oomerrors/","text":"Understanding the \"Out Of Memory\" Errors Whenever you find yourself staring at a stacktrace with an OutOfMemoryError in it, it should all be crystal-clear. The program has got no more elbow room and is dying simply because of the lack of it. From 10,000 feet or an executive chair this might already contain too much information. But for those of you who have to build or maintain the applications and figure out why a particular error is created \u2013 we can share a bit more insight into the issue. In this post we will take a look what do different OutOfMemoryError messages actually mean. We start with the most common cases and move forward to the more interesting situations. java.lang.OutOfMemoryError: Java heap space java.lang.OutOfMemoryError: PermGen space java.lang.OutOfMemoryError: GC overhead limit exceeded java.lang.OutOfMemoryError: unable to create new native thread java.lang.OutOfMemoryError: nativeGetNewTLA java.lang.OutOfMemoryError: Requested array size exceeds VM limit java.lang.OutOfMemoryError: request bytes for . Out of swap space? java.lang.OutOfMemoryError: (Native method) java.lang.OutOfMemoryError: Java heap space We start with the one you have all seen more than you would actually like. This is the Java Virtual Machine\u2019s way to announce you that there is no more room in the virtual machine heap area. You are trying to create a new object, but the amount of memory this newly created structure is about to consume is more than the JVM has in the heap. The JVM has tried to free memory by calling full GC before throwing in the towel, but without any success. The fastest way to get rid of the symptoms is to increase the heap via -Xmx parameter. Note that both this and other recommendations in the article should be taken with a grain of salt. More often than not you just end up hiding the symptoms of the underlying problem. The next suspect is also quite common. java.lang.OutOfMemoryError: PermGen space I guess most of you have seen the java.lang.OutOfMemoryError: PermGen space during redeploys. It is pretty much the same message as the first one, but instead of the heap you are now trying to allocate memory in thePermanent Generation area. And again, you do not have enough room, so the JVM native code is kind enough to let you know about it. This message tends to disappear (for awhile) if you increase the -XX:MaxPermSize parameter. java.lang.OutOfMemoryError: GC overhead limit exceeded The third one \u2013 the java.lang.OutOfMemoryError: GC overhead limit exceeded \u2013 is a bit of a different beast. Instead of the missing heap / permgen the JVM is signaling that your application is spending too much time in garbage collection with little to show for it. By default the JVM is configured to throw this error if you are spending more than 98% of the total time in GC and less than 2% of the heap is recovered after the GC. Sounds like a perfectly good place to have the \u201cfail fast\u201d safeguard at place. In the rare cases where it makes sense to disable it, add -XX:-UseGCOverheadLimit to your startup scripts. The above three OutOfMemoryError messages make up to 98% of the cases Plumbr detects. So there is a strong chance that the remaining quintet is somewhat unknown to you. java.lang.OutOfMemoryError: unable to create new native thread is the message you will receive if the JVM is asking a new thread from the OS and the underlying OS cannot allocate a new thread anymore. This limit is very platform-dependent, so if you are curious to find out your limitations then run your own little experiment using the following code snippet. On my 64bit MacOS X running a latest JDK 7 I run into troubles when creating thread #2032. while(true){ new Thread(new Runnable(){ public void run() { try { Thread.sleep(10000000); } catch(InterruptedException e) { } } }).start(); } java.lang.OutOfMemoryError: nativeGetNewTLA is the symptom where the JVM cannot allocate new Thread Local Area. This is something you only encounter on jRockit virtual machine. If you recall, the Thread Local Area is the buffer used to efficiently allocate memory in a multi-threaded application. Each thread has its own pre-allocated buffer where all the objects instantiated by this thread are born. You will run into problems when you are creating a vast amount of objects in a heavily multi-threaded application, in case of which you might turn to tweaking the -XXtlaSizeparameter . java.lang.OutOfMemoryError: Requested array size exceeds VM limit is the message you find yourself staring at when you are trying to create an array larger than your VM limitations allow. On my 64bit Mac OS X with a recent JDK 7 build I find myself acknowledging the fact that arrays with Integer.MAX_INT-2 elements are OK, but just one more straw, namely Integer.MAX_INT-1, breaks the camel\u2019s back. On older 32-bit machines it had its benefits, limiting the array sizes to fit into the tiny heaps available back then. On modern 64bit machines it seems to create more confusion than to actually help in solving anything. java.lang.OutOfMemoryError: request bytes for . Out of swap space? This error message is thrown when the JVM fails to allocate native memory from the OS. Note that it is completely different from the standard cases where you have exhausted the heap or permgen spaces. This message tends to be displayed when you are operating close to the platform limits. As the message itself is stating you might have exceeded the amount of physical and virtual memory available. As the latter is often implemented via swapping memory to the disk, the first thing you might think of as a quick fix would be to increase the size of the swap file. But I am yet to see an application which would behave normally while swapping, so most likely this quick fix won\u2019t help you much. java.lang.OutOfMemoryError: (Native method) Now it is time to beg for help from your fellow C developers. As the message states, you are facing problems with the native code, but \u2013 unlike in the last case \u2013 the allocation failure was detected in a JNI or native method instead of the JVM code.","title":"Understanding the OutOfMemory Errors"},{"location":"oomerrors/#understanding-the-out-of-memory-errors","text":"Whenever you find yourself staring at a stacktrace with an OutOfMemoryError in it, it should all be crystal-clear. The program has got no more elbow room and is dying simply because of the lack of it. From 10,000 feet or an executive chair this might already contain too much information. But for those of you who have to build or maintain the applications and figure out why a particular error is created \u2013 we can share a bit more insight into the issue. In this post we will take a look what do different OutOfMemoryError messages actually mean. We start with the most common cases and move forward to the more interesting situations. java.lang.OutOfMemoryError: Java heap space java.lang.OutOfMemoryError: PermGen space java.lang.OutOfMemoryError: GC overhead limit exceeded java.lang.OutOfMemoryError: unable to create new native thread java.lang.OutOfMemoryError: nativeGetNewTLA java.lang.OutOfMemoryError: Requested array size exceeds VM limit java.lang.OutOfMemoryError: request bytes for . Out of swap space? java.lang.OutOfMemoryError: (Native method)","title":"Understanding the \"Out Of Memory\" Errors"},{"location":"oomerrors/#javalangoutofmemoryerror-java-heap-space","text":"We start with the one you have all seen more than you would actually like. This is the Java Virtual Machine\u2019s way to announce you that there is no more room in the virtual machine heap area. You are trying to create a new object, but the amount of memory this newly created structure is about to consume is more than the JVM has in the heap. The JVM has tried to free memory by calling full GC before throwing in the towel, but without any success. The fastest way to get rid of the symptoms is to increase the heap via -Xmx parameter. Note that both this and other recommendations in the article should be taken with a grain of salt. More often than not you just end up hiding the symptoms of the underlying problem. The next suspect is also quite common.","title":"java.lang.OutOfMemoryError: Java heap space"},{"location":"oomerrors/#javalangoutofmemoryerror-permgen-space","text":"I guess most of you have seen the java.lang.OutOfMemoryError: PermGen space during redeploys. It is pretty much the same message as the first one, but instead of the heap you are now trying to allocate memory in thePermanent Generation area. And again, you do not have enough room, so the JVM native code is kind enough to let you know about it. This message tends to disappear (for awhile) if you increase the -XX:MaxPermSize parameter.","title":"java.lang.OutOfMemoryError: PermGen space"},{"location":"oomerrors/#javalangoutofmemoryerror-gc-overhead-limit-exceeded","text":"The third one \u2013 the java.lang.OutOfMemoryError: GC overhead limit exceeded \u2013 is a bit of a different beast. Instead of the missing heap / permgen the JVM is signaling that your application is spending too much time in garbage collection with little to show for it. By default the JVM is configured to throw this error if you are spending more than 98% of the total time in GC and less than 2% of the heap is recovered after the GC. Sounds like a perfectly good place to have the \u201cfail fast\u201d safeguard at place. In the rare cases where it makes sense to disable it, add -XX:-UseGCOverheadLimit to your startup scripts. The above three OutOfMemoryError messages make up to 98% of the cases Plumbr detects. So there is a strong chance that the remaining quintet is somewhat unknown to you. java.lang.OutOfMemoryError: unable to create new native thread is the message you will receive if the JVM is asking a new thread from the OS and the underlying OS cannot allocate a new thread anymore. This limit is very platform-dependent, so if you are curious to find out your limitations then run your own little experiment using the following code snippet. On my 64bit MacOS X running a latest JDK 7 I run into troubles when creating thread #2032. while(true){ new Thread(new Runnable(){ public void run() { try { Thread.sleep(10000000); } catch(InterruptedException e) { } } }).start(); }","title":"java.lang.OutOfMemoryError: GC overhead limit exceeded"},{"location":"oomerrors/#javalangoutofmemoryerror-nativegetnewtla","text":"is the symptom where the JVM cannot allocate new Thread Local Area. This is something you only encounter on jRockit virtual machine. If you recall, the Thread Local Area is the buffer used to efficiently allocate memory in a multi-threaded application. Each thread has its own pre-allocated buffer where all the objects instantiated by this thread are born. You will run into problems when you are creating a vast amount of objects in a heavily multi-threaded application, in case of which you might turn to tweaking the -XXtlaSizeparameter .","title":"java.lang.OutOfMemoryError: nativeGetNewTLA"},{"location":"oomerrors/#javalangoutofmemoryerror-requested-array-size-exceeds-vm-limit","text":"is the message you find yourself staring at when you are trying to create an array larger than your VM limitations allow. On my 64bit Mac OS X with a recent JDK 7 build I find myself acknowledging the fact that arrays with Integer.MAX_INT-2 elements are OK, but just one more straw, namely Integer.MAX_INT-1, breaks the camel\u2019s back. On older 32-bit machines it had its benefits, limiting the array sizes to fit into the tiny heaps available back then. On modern 64bit machines it seems to create more confusion than to actually help in solving anything.","title":"java.lang.OutOfMemoryError: Requested array size exceeds VM limit"},{"location":"oomerrors/#javalangoutofmemoryerror-request-bytes-for-out-of-swap-space","text":"This error message is thrown when the JVM fails to allocate native memory from the OS. Note that it is completely different from the standard cases where you have exhausted the heap or permgen spaces. This message tends to be displayed when you are operating close to the platform limits. As the message itself is stating you might have exceeded the amount of physical and virtual memory available. As the latter is often implemented via swapping memory to the disk, the first thing you might think of as a quick fix would be to increase the size of the swap file. But I am yet to see an application which would behave normally while swapping, so most likely this quick fix won\u2019t help you much.","title":"java.lang.OutOfMemoryError: request  bytes for . Out of swap space?"},{"location":"oomerrors/#javalangoutofmemoryerror-native-method","text":"Now it is time to beg for help from your fellow C developers. As the message states, you are facing problems with the native code, but \u2013 unlike in the last case \u2013 the allocation failure was detected in a JNI or native method instead of the JVM code.","title":"java.lang.OutOfMemoryError:   (Native method)"},{"location":"oops/","text":"Java oops Concepts Encapsulation Encapsulation is one of the four fundamental OOP concepts. The other three are inheritance, polymorphism, and abstraction. Encapsulation in Java is a mechanism of wrapping the data (variables) and code acting on the data (methods) together as as single unit. In encapsulation the variables of a class will be hidden from other classes, and can be accessed only through the methods of their current class, therefore it is also known as data hiding. To achieve encapsulation in Java : Declare the variables of a class as private. Provide public setter and getter methods to modify and view the variables values. Inheritance Inheritance can be defined as the process where one class acquires the properties (methods and fields) of another. With the use of inheritance the information is made manageable in a hierarchical order. The class which inherits the properties of other is known as subclass (derived class, child class) and the class whose properties are inherited is known as superclass (base class, parent class). extends is the keyword used to inherit the properties of a class. Below given is the syntax of extends keyword. Polymorphism Polymorphism is the concept where an object behaves differently in different situations. There are two types of polymorphism \u2013 compile time polymorphism and runtime polymorphism. Compile time polymorphism is achieved by method overloading. For example, we can have a class as below. public class Circle { public void draw(){ System.out.println(\"Drwaing circle with default color Black and diameter 1 cm.\"); } public void draw(int diameter){ System.out.println(\"Drwaing circle with default color Black and diameter\"+diameter+\" cm.\"); } public void draw(int diameter, String color){ System.out.println(\"Drwaing circle with color\"+color+\" and diameter\"+diameter+\" cm.\"); } } Here we have multiple draw methods but they have different behavior. This is a case of method overloading because all the methods name is same and arguments are different. Here compiler will be able to identify the method to invoke at compile time, hence it\u2019s called compile time polymorphism. Runtime polymorphism is implemented when we have \u201cIS-A\u201d relationship between objects. This is also called as method overriding because subclass has to override the superclass method for runtime polymorphism. If we are working in terms of superclass, the actual implementation class is decided at runtime. Compiler is not able to decide which class method will be invoked. This decision is done at runtime, hence the name as runtime polymorphism or dynamic method dispatch. package com.test; public interface Shape { public void draw(); } package com.test; public class Circle implements Shape{ @Override public void draw(){ System.out.println(\"Drwaing circle\"); } } package com.test; public class Square implements Shape { @Override public void draw() { System.out.println(\"Drawing Square\"); } } Shape is the superclass and there are two subclasses Circle and Square. Below is an example of runtime polymorphism. Shape sh = new Circle(); sh.draw(); Shape sh1 = getShape(); //some third party logic to determine shape sh1.draw(); In above examples, java compiler don\u2019t know the actual implementation class of Shape that will be used at runtime, hence runtime polymorphism. Abstraction In Object oriented programming Abstraction is a process process of hiding the implementation details from the user, only the functionality will be provided to the user. In other words user will have the information on what the object does instead of how it does it. In Java Abstraction is achieved using Abstract classes and Interfaces. A class which contains the abstract keyword in its declaration is known as abstract class. An interface is a reference type in Java, it is similar to class, it is a collection of abstract methods. A class implements an interface, thereby inheriting the abstract methods of the interface. Along with abstract methods an interface may also contain constants, default methods, static methods, and nested types. Method bodies exist only for default methods and static methods. Writing an interface is similar to writing a class. But a class describes the attributes and behaviours of an object. And an interface contains behaviours that a class implements. Unless the class that implements the interface is abstract, all the methods of the interface need to be defined in the class. An interface is similar to a class in the following ways: An interface can contain any number of methods. An interface is written in a file with a .java extension, with the name of the interface matching the name of the file. The byte code of an interface appears in a .class file. Interfaces appear in packages, and their corresponding bytecode file must be in a directory structure that matches the package name. However, an interface is different from a class in several ways, including: You cannot instantiate an interface. An interface does not contain any constructors. All of the methods in an interface are abstract. An interface cannot contain instance fields. The only fields that can appear in an interface must be declared both static and final. An interface is not extended by a class; it is implemented by a class. An interface can extend multiple interfaces. Refrences https://www.google.com https://www.tutorialspoint.com","title":"Java oops Concepts"},{"location":"oops/#java-oops-concepts","text":"","title":"Java oops Concepts"},{"location":"oops/#encapsulation","text":"Encapsulation is one of the four fundamental OOP concepts. The other three are inheritance, polymorphism, and abstraction. Encapsulation in Java is a mechanism of wrapping the data (variables) and code acting on the data (methods) together as as single unit. In encapsulation the variables of a class will be hidden from other classes, and can be accessed only through the methods of their current class, therefore it is also known as data hiding. To achieve encapsulation in Java : Declare the variables of a class as private. Provide public setter and getter methods to modify and view the variables values.","title":"Encapsulation"},{"location":"oops/#inheritance","text":"Inheritance can be defined as the process where one class acquires the properties (methods and fields) of another. With the use of inheritance the information is made manageable in a hierarchical order. The class which inherits the properties of other is known as subclass (derived class, child class) and the class whose properties are inherited is known as superclass (base class, parent class). extends is the keyword used to inherit the properties of a class. Below given is the syntax of extends keyword.","title":"Inheritance"},{"location":"oops/#polymorphism","text":"Polymorphism is the concept where an object behaves differently in different situations. There are two types of polymorphism \u2013 compile time polymorphism and runtime polymorphism. Compile time polymorphism is achieved by method overloading. For example, we can have a class as below. public class Circle { public void draw(){ System.out.println(\"Drwaing circle with default color Black and diameter 1 cm.\"); } public void draw(int diameter){ System.out.println(\"Drwaing circle with default color Black and diameter\"+diameter+\" cm.\"); } public void draw(int diameter, String color){ System.out.println(\"Drwaing circle with color\"+color+\" and diameter\"+diameter+\" cm.\"); } } Here we have multiple draw methods but they have different behavior. This is a case of method overloading because all the methods name is same and arguments are different. Here compiler will be able to identify the method to invoke at compile time, hence it\u2019s called compile time polymorphism. Runtime polymorphism is implemented when we have \u201cIS-A\u201d relationship between objects. This is also called as method overriding because subclass has to override the superclass method for runtime polymorphism. If we are working in terms of superclass, the actual implementation class is decided at runtime. Compiler is not able to decide which class method will be invoked. This decision is done at runtime, hence the name as runtime polymorphism or dynamic method dispatch. package com.test; public interface Shape { public void draw(); } package com.test; public class Circle implements Shape{ @Override public void draw(){ System.out.println(\"Drwaing circle\"); } } package com.test; public class Square implements Shape { @Override public void draw() { System.out.println(\"Drawing Square\"); } } Shape is the superclass and there are two subclasses Circle and Square. Below is an example of runtime polymorphism. Shape sh = new Circle(); sh.draw(); Shape sh1 = getShape(); //some third party logic to determine shape sh1.draw(); In above examples, java compiler don\u2019t know the actual implementation class of Shape that will be used at runtime, hence runtime polymorphism.","title":"Polymorphism"},{"location":"oops/#abstraction","text":"In Object oriented programming Abstraction is a process process of hiding the implementation details from the user, only the functionality will be provided to the user. In other words user will have the information on what the object does instead of how it does it. In Java Abstraction is achieved using Abstract classes and Interfaces. A class which contains the abstract keyword in its declaration is known as abstract class. An interface is a reference type in Java, it is similar to class, it is a collection of abstract methods. A class implements an interface, thereby inheriting the abstract methods of the interface. Along with abstract methods an interface may also contain constants, default methods, static methods, and nested types. Method bodies exist only for default methods and static methods. Writing an interface is similar to writing a class. But a class describes the attributes and behaviours of an object. And an interface contains behaviours that a class implements. Unless the class that implements the interface is abstract, all the methods of the interface need to be defined in the class. An interface is similar to a class in the following ways: An interface can contain any number of methods. An interface is written in a file with a .java extension, with the name of the interface matching the name of the file. The byte code of an interface appears in a .class file. Interfaces appear in packages, and their corresponding bytecode file must be in a directory structure that matches the package name. However, an interface is different from a class in several ways, including: You cannot instantiate an interface. An interface does not contain any constructors. All of the methods in an interface are abstract. An interface cannot contain instance fields. The only fields that can appear in an interface must be declared both static and final. An interface is not extended by a class; it is implemented by a class. An interface can extend multiple interfaces.","title":"Abstraction"},{"location":"oops/#refrences","text":"https://www.google.com https://www.tutorialspoint.com","title":"Refrences"},{"location":"rssvsz/","text":"Analogy-1 RSS is the Resident Set Size and is used to show how much memory is allocated to that process and is in RAM. It does not include memory that is swapped out. It does include memory from shared libraries as long as the pages from those libraries are actually in memory. It does include all stack and heap memory. VSZ is the Virtual Memory Size. It includes all memory that the process can access, including memory that is swapped out, memory that is allocated, but not used, and memory that is from shared libraries. So if process A has a 500K binary and is linked to 2500K of shared libraries, has 200K of stack/heap allocations of which 100K is actually in memory (rest is swapped or unused), and it has only actually loaded 1000K of the shared libraries and 400K of its own binary then: RSS: 400K + 1000K + 100K = 1500K VSZ: 500K + 2500K + 200K = 3200K Since part of the memory is shared, many processes may use it, so if you add up all of the RSS values you can easily end up with more space than your system has. The memory that is allocated also may not be in RSS until it is actually used by the program. So if your program allocated a bunch of memory up front, then uses it over time, you could see RSS going up and VSZ staying the same. There is also PSS (proportional set size). This is a newer measure which tracks the shared memory as a proportion used by the current process. So if there were two processes using the same shared library from before: PSS: 400K + (1000K/2) + 100K = 400K + 500K + 100K = 1000K Threads all share the same address space, so the RSS, VSZ and PSS for each thread is identical to all of the other threads in the process. Use ps or top to view this information in linux/unix. Analogy-2 This article explains three indicators that can possibly be used to measure the memory consumption of a single process on Linux. VSZ (Virtual Memory Size), RSS (Resident Set Size), and PSS (Proportional Set Size). Although this will lack accuracy, let us consider an allegory to get the idea. There are three people sharing a room. We will consider each person to represent a process, and living expenses to represent memory consumption. Measuring the memory consumption of a single process will be represented as calculating the total living expense for one person in this allegory. Each person has their own cell phone line that is not being shared. All three indicators, VSZ, RSS, and PSS, will count the cell phone bills as each persons living expense individually, and there is no problem with this. The shared room comes with a garage space that can be used if they pay for it, but they all don't drive and they are not using it. However, VSZ will count the entire garage space cost as each persons living expense even though they are not using it. VSZ, therefore, represents the total living expense when they spend on every possible thing regardless of the actual usage. RSS and PSS only count expenses that are actually being used, and therefore, they will not count the garage space cost because it is not being used. Since they are sharing the internet connection and the cable TV, they split up those bills. However, RSS will count the entire amount of the internet connection and cable TV as each persons living expense, even though they are sharing it and splitting up the bill. The idea of RSS is to calculate living expenses as it was not shared with anybody else. PSS will only count one third of the internet connection and cable TV bill as each persons living expense, because they are sharing it. This is more reasonable than RSS since it is considers the fact that they are sharing it. Let's make the allegory a little bit more complicated in order to understand the limitations of PSS. One person is always on the internet and doesn't watch TV that much. Hence, that person pays 50% of the internet connection bill and 20% of the cable TV bill upon agreement. PSS, however, cannot handle situations like this. It will simply calculate one third of the internet connection bill and cable TV bill as each persons living expense. Using PSS is what I consider most reasonable. However, there are limitations and there are also situations where RSS may work better. RSS is reasonable when you want to know the total living expense when you move out and live on your own. Refrences https://www.google.com https://web.archive.org/web/20120520221529/http://emilics.com/blog/article/mconsumption.html http://manpages.ubuntu.com/manpages/en/man1/ps.1.html https://web.archive.org/web/20120520221529/http://emilics.com/blog/article/mconsumption.html","title":"Memory RSS vs VSZ"},{"location":"rssvsz/#analogy-1","text":"RSS is the Resident Set Size and is used to show how much memory is allocated to that process and is in RAM. It does not include memory that is swapped out. It does include memory from shared libraries as long as the pages from those libraries are actually in memory. It does include all stack and heap memory. VSZ is the Virtual Memory Size. It includes all memory that the process can access, including memory that is swapped out, memory that is allocated, but not used, and memory that is from shared libraries. So if process A has a 500K binary and is linked to 2500K of shared libraries, has 200K of stack/heap allocations of which 100K is actually in memory (rest is swapped or unused), and it has only actually loaded 1000K of the shared libraries and 400K of its own binary then: RSS: 400K + 1000K + 100K = 1500K VSZ: 500K + 2500K + 200K = 3200K Since part of the memory is shared, many processes may use it, so if you add up all of the RSS values you can easily end up with more space than your system has. The memory that is allocated also may not be in RSS until it is actually used by the program. So if your program allocated a bunch of memory up front, then uses it over time, you could see RSS going up and VSZ staying the same. There is also PSS (proportional set size). This is a newer measure which tracks the shared memory as a proportion used by the current process. So if there were two processes using the same shared library from before: PSS: 400K + (1000K/2) + 100K = 400K + 500K + 100K = 1000K Threads all share the same address space, so the RSS, VSZ and PSS for each thread is identical to all of the other threads in the process. Use ps or top to view this information in linux/unix.","title":"Analogy-1"},{"location":"rssvsz/#analogy-2","text":"This article explains three indicators that can possibly be used to measure the memory consumption of a single process on Linux. VSZ (Virtual Memory Size), RSS (Resident Set Size), and PSS (Proportional Set Size). Although this will lack accuracy, let us consider an allegory to get the idea. There are three people sharing a room. We will consider each person to represent a process, and living expenses to represent memory consumption. Measuring the memory consumption of a single process will be represented as calculating the total living expense for one person in this allegory. Each person has their own cell phone line that is not being shared. All three indicators, VSZ, RSS, and PSS, will count the cell phone bills as each persons living expense individually, and there is no problem with this. The shared room comes with a garage space that can be used if they pay for it, but they all don't drive and they are not using it. However, VSZ will count the entire garage space cost as each persons living expense even though they are not using it. VSZ, therefore, represents the total living expense when they spend on every possible thing regardless of the actual usage. RSS and PSS only count expenses that are actually being used, and therefore, they will not count the garage space cost because it is not being used. Since they are sharing the internet connection and the cable TV, they split up those bills. However, RSS will count the entire amount of the internet connection and cable TV as each persons living expense, even though they are sharing it and splitting up the bill. The idea of RSS is to calculate living expenses as it was not shared with anybody else. PSS will only count one third of the internet connection and cable TV bill as each persons living expense, because they are sharing it. This is more reasonable than RSS since it is considers the fact that they are sharing it. Let's make the allegory a little bit more complicated in order to understand the limitations of PSS. One person is always on the internet and doesn't watch TV that much. Hence, that person pays 50% of the internet connection bill and 20% of the cable TV bill upon agreement. PSS, however, cannot handle situations like this. It will simply calculate one third of the internet connection bill and cable TV bill as each persons living expense. Using PSS is what I consider most reasonable. However, there are limitations and there are also situations where RSS may work better. RSS is reasonable when you want to know the total living expense when you move out and live on your own.","title":"Analogy-2"},{"location":"rssvsz/#refrences","text":"https://www.google.com https://web.archive.org/web/20120520221529/http://emilics.com/blog/article/mconsumption.html http://manpages.ubuntu.com/manpages/en/man1/ps.1.html https://web.archive.org/web/20120520221529/http://emilics.com/blog/article/mconsumption.html","title":"Refrences"}]}